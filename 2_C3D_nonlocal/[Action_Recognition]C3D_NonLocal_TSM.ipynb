{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"D58BdqotPeRW"},"outputs":[],"source":["# !gdown --id 1Br9F1os2dLkUqwIXwJjByzre2wXTez1W     # 평균 이미지\n","# !gdown --id 1BnU8a7l9tGxZN7wVpeCQx0CIgutW-742     # train/test 분할 정보\n","# !gdown --id 1NyR95VVpq59Z2TKqnUkofHNmGly0ULha     # dataset\n","# !tar -zxvf dataset.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGrKUGPsPeRZ"},"outputs":[],"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import PIL.Image as Image\n","import random\n","import numpy as np\n","import os\n","import os.path\n","from os.path import join\n","import time\n","import pickle\n","import cv2\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.nn import functional as F\n","\n","# Run the code using selected GPU\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 2, 3\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","TRAIN_CHECK_POINT = 'check_point/'\n","\n","# Experiment, Optimization options\n","DATA_SPLIT_PATH = 'data_split.pkl'\n","BATCH_SIZE = 10\n","NUM_CLASSES = 11\n","CROP_SIZE = 112\n","CHANNEL_NUM = 3\n","CLIP_LENGTH = 16\n","EPOCH_NUM = 50\n","LEARNING_RATE = 1e-4"]},{"cell_type":"markdown","metadata":{"id":"k4_juYXGPeRa"},"source":["## Data Processing : Define UCF11Dataset class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iFgl7c6vPeRb"},"outputs":[],"source":["CLIP_LENGTH = 16\n","\n","np_mean = np.load('crop_mean.npy').reshape([CLIP_LENGTH, 112, 112, 3])\n","\n","def get_test_num(filename):\n","    lines = open(filename, 'r')\n","    return len(list(lines))\n","\n","\n","# 비디오 클립의 각 프레임을 처리하는 함수\n","def frame_process(clip, clip_length=CLIP_LENGTH, crop_size=112, channel_num=3):\n","    frames_num = len(clip)\n","    croped_frames = np.zeros([frames_num, crop_size, crop_size, channel_num]).astype(np.float32)\n","\n","    # 모든 프레임을 shape[crop_size, crop_size, channel_num]로 crop\n","    for i in range(frames_num):\n","        img = Image.fromarray(clip[i].astype(np.uint8))\n","\n","        # 이미지의 가로세로 비율을 유지하면서 크기를 조정\n","        if img.width > img.height:\n","            scale = float(crop_size) / float(img.height)\n","            img = np.array(cv2.resize(np.array(img), (int(img.width * scale + 1), crop_size))).astype(np.float32)\n","        else:\n","            scale = float(crop_size) / float(img.width)\n","            img = np.array(cv2.resize(np.array(img), (crop_size, int(img.height * scale + 1)))).astype(np.float32)\n","\n","        # ceter crop\n","        crop_x =\n","        crop_y =\n","        img =\n","\n","        # crop된 이미지에서 평균 이미지를 빼서 정규화\n","        croped_frames[i, :, :, :] = img - np_mean[i]\n","\n","    return croped_frames\n","\n","\n","def convert_images_to_clip(filename, clip_length=CLIP_LENGTH, crop_size=112, channel_num=3):\n","    clip = []\n","    for parent, dirnames, filenames in os.walk(filename):\n","        filenames = sorted(filenames)\n","\n","        # 클립 길이(clip_length)와 폴더 내 파일 수를 비교하여, 필요한 이미지 수를 결정\n","\n","        # 폴더 내의 파일 수가 clip_length보다 작은 경우, 모든 이미지를 로드하고 부족한 만큼 마지막 이미지를 반복하여 추가\n","        if len(filenames) < clip_length:\n","            for i in range(0, len(filenames)):\n","                image_name = str(filename) + '/' + str(filenames[i])\n","                img = Image.open(image_name)\n","                img_data = np.array(img)\n","                clip.append(img_data)\n","            for i in range(clip_length - len(filenames)):\n","                image_name = str(filename) + '/' + str(filenames[len(filenames) - 1])\n","                img = Image.open(image_name)\n","                img_data = np.array(img)\n","                clip.append(img_data)\n","        else:\n","          # 비디오의 길이가 clip_length보다 길어 파일 수가 충분한 경우, 무작위로 시작 인덱스(s_index)를 정하고, 그 위치부터 clip_length만큼의 이미지를 로드해서 clip에 합치기\n","\n","\n","    if len(clip) == 0:\n","        print(filename)\n","    clip = frame_process(clip, clip_length, crop_size, channel_num)\n","    return clip # shape: [clip_length, crop_size, crop_size, channel_num]"]},{"cell_type":"code","source":["class UCF11Dataset(Dataset):\n","    def __init__(self, data_list, num_classes, crop_size=112, channel_num=3):\n","        self.data_list = data_list\n","        self.video_list = list(data_list)\n","        self.crop_size = crop_size\n","        self.channel_num = channel_num\n","        self.num_classes = num_classes\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","    def __getitem__(self, i):\n","        line = self.video_list[i].strip('\\n').split()\n","        dirname = line[0]\n","        label = int(self.data_list[dirname])\n","        clips = convert_images_to_clip(dirname, CLIP_LENGTH, self.crop_size, self.channel_num)\n","\n","        clips = np.transpose(np.array(clips).astype(np.float32), (3, 0, 1, 2))  # shape: (channel_num, clip_length, crop_size, crop_size)\n","\n","        batch_data = {'clips': clips, 'labels': label}\n","\n","        return batch_data"],"metadata":{"id":"3MsPOVZ76i5K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6sEeYYyhPeRc"},"source":["## Load UCF11(UCF YouTube Action) Dataset Path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpQNTPOGPeRc"},"outputs":[],"source":["DATA_SPLIT_PATH = 'data_split.pkl'\n","ucf11_dataset = pickle.load(open(DATA_SPLIT_PATH,'rb'))\n","train_set = ucf11_dataset['train']\n","test_set = ucf11_dataset['test']"]},{"cell_type":"markdown","metadata":{"id":"RKIwX9-IPeRc"},"source":["## Set Dataset and Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbQsWiJAPeRc"},"outputs":[],"source":["train_video_dataset = UCF11Dataset(train_set, NUM_CLASSES)\n","test_video_dataset = UCF11Dataset(test_set, NUM_CLASSES)\n","\n","train_video_dataloader = DataLoader(train_video_dataset, batch_size = BATCH_SIZE, shuffle=True)\n","test_video_dataloader = DataLoader(test_video_dataset, batch_size = BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"RVF5AlNRPeRd"},"source":["## Define NonLocal Block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEWnFfWcPeRd"},"outputs":[],"source":["class NonLocalBlock3D(nn.Module):\n","    def __init__(self, in_channels, test_mode=False, dimension=3, sub_sample=True):\n","        super(NonLocalBlock3D, self).__init__()\n","\n","        self.test_mode = test_mode\n","        self.dimension = dimension\n","        self.sub_sample = sub_sample\n","\n","        self.in_channels = in_channels\n","\n","        self.inter_channels = in_channels // 2\n","        if self.inter_channels == 0:\n","            self.inter_channels = 1\n","\n","        max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n","\n","        #============================================================\n","        #make self.g , self.theta, self.phi\n","        #these are nn.Conv3d, 1x1x1, stride=1, padding=0\n","        #============================================================\n","        self.g =\n","\n","        self.theta =\n","\n","        self.phi =\n","        #============================================================\n","\n","        #============================================================\n","        #make self.W\n","        #in this part, self.W.weight and self.W.bias must initialize to 0\n","        #============================================================\n","        self.W =\n","        nn.init.constant_(self.W.bias, 0)\n","        #============================================================\n","\n","        if sub_sample:\n","            self.g = nn.Sequential(self.g, max_pool_layer)\n","            self.phi = nn.Sequential(self.phi, max_pool_layer)\n","\n","    def forward(self, x):\n","        '''\n","        :param x: (b, c, t, h, w)\n","        :return:\n","        '''\n","        batch_size = x.size(0)\n","        #============================================================\n","        #1. use self.g(x)\n","        #2. use self.theta(x)\n","        #3. use self.phi(x)\n","        #4. several matrix multiplication between previous return value\n","        #5. use self.W(y)\n","        #6. make z with x and self.W(y)\n","        #============================================================\n","        g_x =\n","        g_x = g_x.permute(0, 2, 1)\n","\n","        theta_x =\n","        theta_x = theta_x.permute(0, 2, 1)\n","\n","        phi_x =\n","\n","        f =\n","        f_div_C =\n","\n","        y =\n","        if self.test_mode:\n","            print(\"x: {}\".format(x.shape))\n","            print(\"g_x: {}\".format(g_x.shape))\n","            print(\"theta_x: {}\".format(theta_x.shape))\n","            print(\"phi_x: {}\".format(phi_x.shape))\n","            print(\"f: {}\".format(f.shape))\n","            print(\"y: {}\".format(y.shape))\n","\n","        y = y.permute(0, 2, 1).contiguous()\n","        y =\n","        W_y =\n","        z =\n","        #============================================================\n","\n","        return z"]},{"cell_type":"markdown","metadata":{"id":"6Ry87jKDPeRd"},"source":["## Define C3D Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJnv9kRuPeRd"},"outputs":[],"source":["class C3D(nn.Module):\n","    \"\"\"\n","    The C3D network.\n","    \"\"\"\n","\n","    def __init__(self, num_classes, pretrained=\"\"):\n","        super(C3D, self).__init__()\n","\n","        #============================================================\n","        #All of convolution layers use kernel_size (3,3,3) and padding (1, 1, 1)\n","        #conv1 3 -> 64\n","        #conv2 64 -> 128\n","        #conv3a 128 -> 256\n","        #conv3b 256 -> 256\n","        #conv4a 256 -> 512\n","        #conv4b 512 -> 512\n","        #conv5a 512 -> 512\n","        #conv5b 512 -> 512\n","        #fc6 (you need to find input channel size) -> 4096\n","        #fc7 4096 -> num_classes\n","        #============================================================\n","\n","        self.conv1 =\n","        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n","        self.nonlocal1 = NonLocalBlock3D(64)\n","\n","        self.conv2 =\n","        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","        self.nonlocal2 = NonLocalBlock3D(128)\n","\n","        self.conv3a =\n","        self.conv3b =\n","        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","        self.nonlocal3 = NonLocalBlock3D(256)\n","\n","        self.conv4a =\n","        self.conv4b =\n","        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","        self.nonlocal4 = NonLocalBlock3D(512)\n","\n","        self.conv5a =\n","        self.conv5b =\n","        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n","\n","        self.fc6 =\n","        self.fc7 =\n","        #============================================================\n","\n","        self.dropout = nn.Dropout(p=0.5)\n","\n","        self.relu = nn.ReLU()\n","\n","        self.__init_weight()\n","\n","        if pretrained:\n","            self.__load_pretrained_weights(pretrained)\n","\n","    def forward(self, x):\n","\n","        #============================================================\n","        #use all layer to forward\n","        #============================================================\n","\n","\n","        #============================================================\n","        logits = self.fc7(x)\n","\n","        return logits\n","\n","    def __load_pretrained_weights(self, model_path):\n","        \"\"\"Initialiaze network.\"\"\"\n","        corresp_name = {\n","                        # Conv1\n","                        \"features.0.weight\": \"conv1.weight\",\n","                        \"features.0.bias\": \"conv1.bias\",\n","                        # Conv2\n","                        \"features.3.weight\": \"conv2.weight\",\n","                        \"features.3.bias\": \"conv2.bias\",\n","                        # Conv3a\n","                        \"features.6.weight\": \"conv3a.weight\",\n","                        \"features.6.bias\": \"conv3a.bias\",\n","                        # Conv3b\n","                        \"features.8.weight\": \"conv3b.weight\",\n","                        \"features.8.bias\": \"conv3b.bias\",\n","                        # Conv4a\n","                        \"features.11.weight\": \"conv4a.weight\",\n","                        \"features.11.bias\": \"conv4a.bias\",\n","                        # Conv4b\n","                        \"features.13.weight\": \"conv4b.weight\",\n","                        \"features.13.bias\": \"conv4b.bias\",\n","                        # Conv5a\n","                        \"features.16.weight\": \"conv5a.weight\",\n","                        \"features.16.bias\": \"conv5a.bias\",\n","                         # Conv5b\n","                        \"features.18.weight\": \"conv5b.weight\",\n","                        \"features.18.bias\": \"conv5b.bias\",\n","                        # fc6\n","                        \"classifier.0.weight\": \"fc6.weight\",\n","                        \"classifier.0.bias\": \"fc6.bias\",\n","                        # fc7\n","                        \"classifier.3.weight\": \"fc7.weight\",\n","                        \"classifier.3.bias\": \"fc7.bias\",\n","                        }\n","\n","        p_dict = torch.load(model_path)['state_dict']\n","        s_dict = self.state_dict()\n","        for name in p_dict:\n","            if name not in corresp_name:\n","                continue\n","            s_dict[corresp_name[name]] = p_dict[name]\n","        self.load_state_dict(s_dict)\n","\n","    def __init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                # m.weight.data.normal_(0, math.sqrt(2. / n))\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm3d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()"]},{"cell_type":"markdown","metadata":{"id":"f-LfNAyYPeRe"},"source":["## Set Network and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKwYwfPnPeRe"},"outputs":[],"source":["net = C3D(num_classes=NUM_CLASSES)\n","net = net.cuda()\n","\n","#net = C3D(num_classes=NUM_CLASSES).cuda()\n","#net = torch.nn.DataParallel(net).to(device)\n","\n","optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{"id":"4Aw8618tPeRe"},"source":["## Train and Test C3D"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"GmvJEYEXPeRe"},"outputs":[],"source":["for epoch in range(EPOCH_NUM):\n","    # train\n","    correct_epoch = 0\n","    loss_epoch = 0\n","    net.train()\n","\n","    for i, batch in enumerate(train_video_dataloader):\n","        batch_clips = batch['clips']\n","        batch_labels = batch['labels']\n","        batch_clips = batch_clips.cuda()\n","        batch_labels = batch_labels.cuda()\n","\n","        logits = net(batch_clips)\n","\n","        loss = F.cross_entropy(logits, batch_labels)\n","        correct = (torch.argmax(logits, 1) == batch_labels).sum()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss_epoch += loss\n","        correct_epoch += correct\n","\n","        if i % 10 == 0:\n","            print('Epoch %d, Batch %d: Loss is %.5f; Accuracy is %.5f'%(epoch+1, i, loss, correct/batch_clips.shape[0]))\n","\n","    print('Epoch %d: Average loss is: %.5f; Average accuracy is: %.5f'%(epoch+1, loss_epoch / len(train_video_dataloader),\n","                                                                                correct_epoch / len(train_video_dataset)))\n","\n","    # test\n","    correct_epoch = 0\n","    loss_epoch = 0\n","    net.eval()\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(test_video_dataloader):\n","            batch_clips = batch['clips']\n","            batch_labels = batch['labels']\n","            batch_clips = batch_clips.cuda()\n","            batch_labels = batch_labels.cuda()\n","\n","            logits = net(batch_clips)\n","\n","            loss = F.cross_entropy(logits, batch_labels)\n","            correct = (torch.argmax(logits, 1) == batch_labels).sum()\n","\n","            loss_epoch += loss\n","            correct_epoch += correct\n","\n","    print('Test loss is %.5f; Accuracy is %.5f'%(loss_epoch / len(test_video_dataloader),\n","                                                                                correct_epoch / len(test_video_dataset)))\n"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","CLASS_LABELS = [\n","    \"basketball\", \"biking\", \"diving\", \"golf_swing\", \"horse_riding\",\n","    \"soccer_juggling\", \"swing\", \"tennis_swing\", \"trampoline_jumping\",\n","    \"volleyball_spiking\", \"walking\"\n","]\n","\n","def restore_processed_frame(processed_frame, frame_mean):\n","    restored_frame = np.transpose(processed_frame, (1, 2, 0))\n","    restored_frame = restored_frame + frame_mean\n","    restored_frame = np.clip(restored_frame, 0, 255)\n","    restored_frame = restored_frame.astype(np.uint8)\n","    return restored_frame\n","\n","def predict(net, clip):\n","    logits = net(torch.tensor(clip).unsqueeze(dim=0).cuda())\n","    predicted_class_index =  torch.argmax(logits, dim=1).item()\n","    return CLASS_LABELS[predicted_class_index]\n","\n","def display_restored_clip(processed_clip, np_mean, true_label, predicted_label):\n","    num_frames = processed_clip.shape[1]\n","    rows = (num_frames + 3) // 4\n","\n","    fig, axes = plt.subplots(rows, 4, figsize=(20, 5 * rows))\n","    fig.suptitle(f'True Label: {true_label}\\nPredicted Label: {predicted_label}',\n","                 fontsize=32, y=1.02)\n","\n","    for i in range(num_frames):\n","        row = i // 4\n","        col = i % 4\n","\n","        restored_frame = restore_processed_frame(processed_clip[:, i, :, :], np_mean[i])\n","\n","        if rows > 1:\n","            ax = axes[row, col]\n","        else:\n","            ax = axes[col]\n","\n","        ax.imshow(restored_frame)\n","        ax.set_title(f'Frame {i+1}', fontsize=12)\n","        ax.axis('off')\n","\n","    for i in range(num_frames, rows * 4):\n","        row = i // 4\n","        col = i % 4\n","        if rows > 1:\n","            fig.delaxes(axes[row, col])\n","        else:\n","            fig.delaxes(axes[col])\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def visualize_sample(dataset, index, model):\n","    sample = dataset[index]\n","    processed_clip = sample['clips']\n","    true_label_index = sample['labels']\n","    true_label = CLASS_LABELS[true_label_index]\n","\n","    predicted_label = predict(net, processed_clip)\n","\n","    display_restored_clip(processed_clip, np_mean, true_label, predicted_label)\n","\n","\n","\n","visualize_sample(test_video_dataset, 0, net)"],"metadata":{"id":"iocvwtdmpCnA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define TSM-ResNet50"],"metadata":{"id":"WuQkHjVEFAru"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","\n","class TemporalShift(nn.Module):\n","    def __init__(self, net, n_segment=16, n_div=8):\n","        super(TemporalShift, self).__init__()\n","        self.net = net\n","        self.n_segment = n_segment\n","        self.fold_div = n_div\n","\n","    def forward(self, x):\n","        x = self.shift(x, self.n_segment, fold_div=self.fold_div)\n","        return self.net(x)\n","\n","    @staticmethod\n","    def shift(x, n_segment, fold_div=8):\n","        nt, c, h, w = x.size()  # nt는 n (배치 크기) * t (시간 세그먼트)\n","        n_batch = nt // n_segment\n","        x = x.view(n_batch, n_segment, c, h, w)\n","\n","        fold = c // fold_div\n","        out = torch.zeros_like(x)\n","\n","        #============================================================\n","        # Complete the Temporal Shift operation\n","        # Hint: You need to shift a portion of the channels to the left and another portion to the right\n","        #============================================================\n","\n","\n","\n","        #============================================================\n","\n","        return out.view(nt, c, h, w)\n","\n","\n","\n","class TSM_ResNet(nn.Module):\n","    def __init__(self, num_classes, n_segment=16):\n","        super(TSM_ResNet, self).__init__()\n","        self.n_segment = n_segment\n","        self.base_model = models.resnet50(pretrained=True)\n","\n","        #============================================================\n","        # Apply TemporalShift module to each ResNet layer\n","        # You can access each layer of the ResNet model using syntax like self.base_model.layer1\n","        #============================================================\n","\n","\n","        # Shift left\n","        out[:, :-1, :fold] = x[:, 1:, :fold]  # 채널의 일부를 왼쪽으로 이동\n","\n","        # Shift right\n","        out[:, 1:, fold: 2 * fold] = x[:, :-1, fold: 2 * fold]  # 채널의 일부를 오른쪽으로 이동\n","\n","        # No shift\n","        out[:, :, 2 * fold:] = x[:, :, 2 * fold:]  # 나머지 채널은 그대로 유지\n","\n","\n","        #============================================================\n","\n","        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, channels, num_segments, height, width)\n","        b, c, t, h, w = x.size()\n","        x = x.permute(0, 2, 1, 3, 4).contiguous()  # (b, t, c, h, w)\n","\n","        #============================================================\n","        # Reshape the input tensor for 2D CNN\n","        x = x.view(b * t, c, h, w)\n","        #============================================================\n","\n","        x = self.base_model.conv1(x)\n","        x = self.base_model.bn1(x)\n","        x = self.base_model.relu(x)\n","        x = self.base_model.maxpool(x)\n","\n","        x = self.base_model.layer1(x)\n","        x = self.base_model.layer2(x)\n","        x = self.base_model.layer3(x)\n","        x = self.base_model.layer4(x)\n","\n","        x = self.base_model.avgpool(x)\n","        x = x.view(b, t, -1).mean(1)\n","        x = self.base_model.fc(x)\n","        return x"],"metadata":{"id":"QR223edLFBIv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = TSM_ResNet(num_classes=NUM_CLASSES)\n","net = net.cuda()\n","\n","optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)"],"metadata":{"id":"O7abc4eDFRvm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train and Test TSM"],"metadata":{"id":"hLQPZKL6FYCG"}},{"cell_type":"code","source":["for epoch in range(EPOCH_NUM):\n","    # train\n","    correct_epoch = 0\n","    loss_epoch = 0\n","    net.train()\n","\n","    for i, batch in enumerate(train_video_dataloader):\n","        batch_clips = batch['clips']\n","        batch_labels = batch['labels']\n","        batch_clips = batch_clips.cuda()\n","        batch_labels = batch_labels.cuda()\n","\n","        logits = net(batch_clips)\n","\n","        loss = F.cross_entropy(logits, batch_labels)\n","        correct = (torch.argmax(logits, 1) == batch_labels).sum()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss_epoch += loss\n","        correct_epoch += correct\n","\n","        if i % 10 == 0:\n","            print('Epoch %d, Batch %d: Loss is %.5f; Accuracy is %.5f'%(epoch+1, i, loss, correct/batch_clips.shape[0]))\n","\n","    print('Epoch %d: Average loss is: %.5f; Average accuracy is: %.5f'%(epoch+1, loss_epoch / len(train_video_dataloader),\n","                                                                                correct_epoch / len(train_video_dataset)))\n","\n","    # test\n","    correct_epoch = 0\n","    loss_epoch = 0\n","    net.eval()\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(test_video_dataloader):\n","            batch_clips = batch['clips']\n","            batch_labels = batch['labels']\n","            batch_clips = batch_clips.cuda()\n","            batch_labels = batch_labels.cuda()\n","\n","            logits = net(batch_clips)\n","\n","            loss = F.cross_entropy(logits, batch_labels)\n","            correct = (torch.argmax(logits, 1) == batch_labels).sum()\n","\n","            loss_epoch += loss\n","            correct_epoch += correct\n","\n","    print('Test loss is %.5f; Accuracy is %.5f'%(loss_epoch / len(test_video_dataloader),\n","                                                                                correct_epoch / len(test_video_dataset)))a"],"metadata":{"id":"rT5nfcdPFTdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define NonLocal-ResNet50"],"metadata":{"id":"qK_pfnBwIAXe"}},{"cell_type":"code","source":["class NonLocal_ResNet(nn.Module):\n","    def __init__(self, num_classes, n_segment=16):\n","        super(NonLocal_ResNet, self).__init__()\n","        self.n_segment = n_segment\n","        self.base_model = models.resnet50(pretrained=True)\n","\n","        # Get the number of output channels for each layer\n","        self.layer1_channels = self.base_model.layer1[-1].conv3.out_channels\n","        self.layer2_channels = self.base_model.layer2[-1].conv3.out_channels\n","        self.layer3_channels = self.base_model.layer3[-1].conv3.out_channels\n","        self.layer4_channels = self.base_model.layer4[-1].conv3.out_channels\n","\n","        #============================================================\n","        # Add Non-local blocks after each ResNet block\n","        #============================================================\n","\n","        self.non_local1 = NonLocalBlock3D(self.layer1_channels)\n","        self.non_local2 = NonLocalBlock(self.layer2_channels)\n","        self.non_local3 = NonLocalBlock(self.layer3_channels)\n","        self.non_local4 = NonLocalBlock(self.layer4_channels)\n","\n","        #============================================================\n","\n","        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, channels, num_segments, height, width)\n","        b, c, t, h, w = x.size()\n","        x = x.permute(0, 2, 1, 3, 4).contiguous()  # (b, t, c, h, w)\n","        x = x.view(b * t, c, h, w)\n","\n","        x = self.base_model.conv1(x)\n","        x = self.base_model.bn1(x)\n","        x = self.base_model.relu(x)\n","        x = self.base_model.maxpool(x)\n","\n","        x = self.base_model.layer1(x)\n","        x = x.view(b, t, x.size(1), x.size(2), x.size(3)).transpose(1, 2).contiguous()\n","        x = self.non_local1(x)\n","        x = x.transpose(1, 2).contiguous().view(b*t, -1, x.size(3), x.size(4))\n","\n","        x = self.base_model.layer2(x)\n","        x = x.view(b, t, x.size(1), x.size(2), x.size(3)).transpose(1, 2).contiguous()\n","        x = self.non_local2(x)\n","        x = x.transpose(1, 2).contiguous().view(b*t, -1, x.size(3), x.size(4))\n","\n","        x = self.base_model.layer3(x)\n","        x = x.view(b, t, x.size(1), x.size(2), x.size(3)).transpose(1, 2).contiguous()\n","        x = self.non_local3(x)\n","        x = x.transpose(1, 2).contiguous().view(b*t, -1, x.size(3), x.size(4))\n","\n","        x = self.base_model.layer4(x)\n","        x = x.view(b, t, x.size(1), x.size(2), x.size(3)).transpose(1, 2).contiguous()\n","        x = self.non_local4(x)\n","        x = x.transpose(1, 2).contiguous().view(b*t, -1, x.size(3), x.size(4))\n","\n","        x = self.base_model.avgpool(x)\n","        x = x.view(b, t, -1).mean(1)\n","        x = self.base_model.fc(x)\n","        return x"],"metadata":{"id":"iI9C-wUiIE9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = NonLocal_ResNet(num_classes=NUM_CLASSES)\n","net = net.cuda()\n","\n","optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)"],"metadata":{"id":"2EIb7gqeIKco"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(EPOCH_NUM):\n","    # train\n","    correct_epoch = 0\n","    loss_epoch = 0\n","    net.train()\n","\n","    for i, batch in enumerate(train_video_dataloader):\n","        batch_clips = batch['clips']\n","        batch_labels = batch['labels']\n","        batch_clips = batch_clips.cuda()\n","        batch_labels = batch_labels.cuda()\n","\n","        logits = net(batch_clips)\n","\n","        loss = F.cross_entropy(logits, batch_labels)\n","        correct = (torch.argmax(logits, 1) == batch_labels).sum()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss_epoch += loss\n","        correct_epoch += correct\n","\n","        if i % 10 == 0:\n","            print('Epoch %d, Batch %d: Loss is %.5f; Accuracy is %.5f'%(epoch+1, i, loss, correct/batch_clips.shape[0]))\n","\n","    print('Epoch %d: Average loss is: %.5f; Average accuracy is: %.5f'%(epoch+1, loss_epoch / len(train_video_dataloader),\n","                                                                                correct_epoch / len(train_video_dataset)))\n","\n","    # test\n","    correct_epoch = 0\n","    loss_epoch = 0\n","    net.eval()\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(test_video_dataloader):\n","            batch_clips = batch['clips']\n","            batch_labels = batch['labels']\n","            batch_clips = batch_clips.cuda()\n","            batch_labels = batch_labels.cuda()\n","\n","            logits = net(batch_clips)\n","\n","            loss = F.cross_entropy(logits, batch_labels)\n","            correct = (torch.argmax(logits, 1) == batch_labels).sum()\n","\n","            loss_epoch += loss\n","            correct_epoch += correct\n","\n","    print('Test loss is %.5f; Accuracy is %.5f'%(loss_epoch / len(test_video_dataloader),\n","                                                                                correct_epoch / len(test_video_dataset)))"],"metadata":{"id":"Unro8h23IL2G"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9 (default, Nov 25 2022, 14:10:45) \n[GCC 8.4.0]"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}